4.

EPISTEMIC DEPTH AND AWARENESS
â€œ...my own working hypothesis is that consciousness is our inner model of an "epistemic space,"
a space in which possible and actual states of knowledge can be represented. I think that conscious
beings are precisely those who have a model of their own space of knowledgeâ€”they are systems
that (in an entirely nonlinguistic and nonconceptual way) know that they currently have the
capacity to know something.â€4
-

Metzinger, 2020

4

We generally agree with Metzinger's (2020; 2024) emphasis on an epistemic "space", though we prefer the term field because it
highlights that the field and its contents are 'one' and always changing.

9

The word epistemic means 'relating to knowledge' and the word 'depth' refers to both intensity and
the capacity to go below or beyond the surface (Oxford English, 1989). What we mean by the term epistemic
depth is therefore a capacity or continuum (i.e., deepening) of knowing, or awareness, that can be more or
less active (i.e., intense or clear). A state of low epistemic depth is one that involves unclear knowing, such
as states of sleeping, dreaming, or mind-wandering, and a state with high epistemic depth is one that
involves clear or intense knowing, such as mindful or highly aware states (Schooler, 2002; Schooler et al.,
2011). As we will see, the intensity or clarity of knowing can also be directed at the knowing capacity itself,
i.e., reflectively knowing that we know (Dunne et al., 2019; Josipovic, 2019). Our goal in this section is to
deliver a conceptual sense of what we mean by epistemic depth. We then focus on providing a formalization
of this idea in Section 5.
To add some phenomenological nuance to our construct we borrow the term luminosity, which appears
often in the ancient discourses of contemplative traditions (AnÄlayo 2017), particularly in Mahayana and
Vajrayana Buddhism (Williams, 2013) and Indian philosophy (Skorupski, 2012; Berger, 2015). For our
purposes we define luminosity as the clarity or intensity of knowing or awareness within conscious
experience. Connecting epistemic depth with luminosity has the particular benefit of avoiding an infinite
regress: "as a source of light is never illuminated by another oneâ€¦" (Bhartá¹›hari, 1963). Just as a light
shining out of a lamp illuminates the objects but also the lamp itself, ideas of luminosity and self-reflective
awareness often go hand in hand (Williams, 2013). Luminosity and recursion indicate that it is just 'one'
knowing with varying degrees, just as a light varies in brightness, but is one light. For our purposes,
luminosity provides a useful metaphorâ€”with phenomenological resonanceâ€”for the graded nature or
clarity of awareness that seems to be possible for conscious systems.
Now, returning to our construct of the reality model. In this context, luminosity is the degree to which
the reality model (non-locally) knows itself. Within a hierarchical active inference system, the requisite
sharing means that the reality model entails the inference, belief, or expectation, that it exists. By metaphor,
it is as if the system's output becomes another sensory modality that is recursively distributed back through
all layers of the system. To provide a metaphor: When we speak out loud, we produce sounds and at the
same time hear those sounds and their meaning (i.e., we hear our own voice and what we are saying).
Therefore, our output (voice) is also our input (sound). We sequentially produce form (output) and then
monitor the global context of that form (input) to ensure that our speech communicates a coherent stream
of meaning5. There is a continuous 'looping' between what we create through action and what we perceive
through the senses. Similarly, the key output of the inferential process in the brain is the construction of a
reality model that allows us to survive (analogous to the voice). But this global reality model is also an
input to the system and becomes part of the inferential process itself (analogous to the sound, cf. Figure 3).
Consider that while particular contents of the reality model can be confirmed or disconfirmed by new
evidence (e.g., transitions in binocular rivalry), the existence of the reality model is nevertheless receiving
5

A more nuanced treatment of this analogy would call upon sensory attenuation; namely, the attenuation of the
precision of the sensory consequences of self generated outputs. Following action, sensory attenuation is suspended
so that we can attend to the consequences of what we have just done. This is clearly evinced in saccadic eye
movements, where sensory attenuation is known as saccadic suppression, while the sensory attention â€” during
successive fixations â€” underwrites epistemic foraging of a visual scene required to construct a perceptual gestalt.

10

continuous validation, regardless of the contents (e.g., all changes confirm that a reality model exists).
Hence, the epistemic field is constantly evidencing its own existence (i.e., field-evidencing). Any action the
organism takesâ€”as little as a saccade, a thought, or a breathâ€”confirms to itself that it (the model) exists.
Indeed, all model (i.e., Bayesian belief) updates confirm it. Hence, the fact that the reality model exists
becomes a precise inference that rarely loses the inferential competition.
Figure 3
Generating an epistemic field and its reflective sharing

EPISTEMIC DEPTH

EN
TIA
LC
OM
IN
FE
R

WINNING INFERENCES BIND INTO THE GLOBAL POSTERIOR:
THE WORLD MODEL

PE
TIT
I

ON

Ã˜ The world model reï¬‚exively feeds back into the abstraction hierarchy
Ã˜ The broadcast information enhances the binding process by upweighting
representations that cohere with the uniï¬ed world model

Note. This figure illustrates the integration of information (operationalized by the hierarchical generative model (HGM)) into a
reality model via nested Bayesian binding. The cone at the center illustrates a multi-tiered HGM structure with increasing levels of
abstraction, from basic unimodal processes to abstract reasoning exemplified by large scale networks in the brain (Taylor et al.,
2015). The cone includes feedforward and feedback loops throughout all layers. Increasing abstraction reflects increasing
compression, information integration, temporal depth, and conceptualization (cf. Figure 1). A weighted combination of features
across the hierarchy are combined or bound together via inferential competition (faded green arrows) to form a global posterior
which is homologous to the reality model (the "conscious cloud" on the top left). This conscious cloud contains diverse perceptual,
sensory, and conceptual elements, connected to corresponding hierarchical levels. Crucially, the reality model is recursively
broadcast back throughout the hierarchy in the form of top-down predictions of both content and context (thick green arrow), where
context is instantiated by predictions of precision. Crucially, predictions of precision weight the prediction errors that underwrite
those predictions in a recursive fashion. This sharing of the reality model fine-tunes inference for binding by upweighting
representations that cohere with it. We hypothesize that this recursion is the causal mechanism permitting epistemic depth (the
sensation of knowing) because the information contained in the reality model loops back into the "conscious cloud" via the implicit
abstraction hierarchy. Hence, the reality model contains information about the existence of itself. While the 'loop' is shown to and
from the conscious cloud to illustrate the schema, computationally, all the recursion is within the feedback loops of the central cone
structure: there is no dualism implied in this account.

11

5.

HYPER-MODELS AND TINY CREATURES

Modeling epistemic depth in a rigorous way calls for a system that not only forms predictions about
external states but alsoâ€”cruciallyâ€”models its own modeling recursively at a global scale. One way to
pursue this is through what we term hyper-generative models, or hyper-models for short (Friston, 2010;
Parr & Friston, 2018; Ramstead et al., 2022). In hierarchical active inference, each layer infers hidden
causes in ascending degrees of abstraction. However, to capture epistemic depth, the architecture requires
a truly higher-order (i.e., hyper) model that tracks how each layer's inferences and precision-weightings
are being deployed system-wide. Formally, we can posit a hyper-parameter set Î¦ that encodes beliefs about
which layers to trust more (or less) under different contexts, how strongly to up- or down-weight prediction
errors, and how to orchestrate feedback loops across the entire network (Friston et al., 2017). Such a deeply
global parameter permits a system to recursively "rework" and "rediscover" their own modelling processes,
and thus become a truly agentic self-constructing and deconstructing system, reminiscent of the way
humans can intentionally and radically change themselves given the right motivation and context.
Crucially, hyper-parameters that contextualize belief updatingâ€”through descending predictions of
precision to lower layersâ€”update the (ascending) precision-weighted prediction errors that update the
hyper-parameters that update (descending) predictions of precision. And so on, ad infinitum. It is this
recursive aspect that equips belief updating with epistemic depth. In terms of phenomenal transparency and
opacity, we can imagine each hierarchical layer as a type of glass that can change its optical properties (cf.
Figure 4). In the setting of epistemic depth, descending predictions of precision render transparent panes of
glass opaque, equipping the hierarchy with the ability to contextualize and select what is broadcast from
one level to the next. In terms of a lamp illuminating itself, epistemic depth offers a very different picture:
a picture more akin to a series of holographic screens (Fields et al., 2021; Fields et al., 2024) illuminating
each other in their reflected light. This picture foregrounds the recursive, non-local and (self) reflective
nature of epistemic depth.
Clarifying how a hyper-parameter set, Î¦ orchestrates the entire system is a challenge. One possibility
is to define Î¦ within a factor-graph architecture that includes "hyper-nodes" encoding conditional beliefs
about each sub-model's precision or reliability (Parr & Friston, 2018). These hyper-nodes would propagate
top-down signalsâ€”precision updates, gating directives, or structural reconfigurationsâ€”to lower-level
nodes, ensuring that each layer's inference is shaped by global meta-beliefs. Such a mechanism would allow
simulation of when and how reflective broadcasts occur, enabling comparisons to neurophysiological data
and refining our broader understanding of epistemic depth in biological and artificial systems.
Practically, this kind of architecture has proved useful in modelling brain responses (Iglesias et al.,
2013), using a variant of predictive coding called the hierarchical Gaussian filter (Mathys et al., 2011). In
computational neuroscience, minimal forms of epistemic depth have been used to illustrate attentional
selection and the segregation of figure from ground (Kanai et al., 2015). Technically, the nonlocal aspect
of epistemic depth inherits from the fact that the hyper-parameterâ€”prescribing precisions at every level of
the hierarchyâ€”renders each level part of the hyper-parameter's Markov blanket (because they are all
children of the hyper-parameter). This mandates recursive message passing between the Bayesian beliefs

over hyper-parameters and all levels, in which descending predictions of precision are reflected back in the
form of a prediction error over precision. See Kanai et al., (2015) for the functional form of these secondorder prediction errors in the context of predictive coding architectures.
Figure 4
Epistemic depth as hyper-modeling
Global Hyper-Model & Minimization of Hyper Free-Energy

Hierarchy of latent states ğ‘¥ (1) , â€¦ , ğ‘¥ (ğ¿)

ğ¿âˆ’1

Lower layers represent concrete features, while
higher layers encode abstract patterns, forming the
uniï¬ed reality model essential for experience.

ğ‘ ğ‘ , ğ‘¥ (1), â€¦ , ğ‘¥ ğ¿ , Î¦ = ğ‘ ğ‘ |ğ‘¥ (1)

ğ‘ ğ‘¥ (ğ‘™)|ğ‘¥ ğ‘™+1 , ğœ™ (ğ‘™)

ğ‘ ğ‘¥ (ğ¿)|ğœ™ ğ¿ ğ‘ Î¦

ğ‘™=1

ğ¹hyper = ğ”¼ğ‘ ğ‘¥ (ğ‘™) , â€¦ ,ğ‘¥ (ğ‘™+1) ,Î¦ ln ğ‘ ğ‘¥ (ğ‘™) , â€¦ , ğ‘¥ (ğ‘™+1) , Î¦ âˆ’ ln ğ‘ ğ‘ , ğ‘¥ (1) , â€¦ , ğ‘¥ ğ¿ , Î¦

Mğ®ğ¥ğ­ğ¢ğ¥ğšğ²ğğ« Generative Process
ğ¿âˆ’1

ğ‘ ğ‘ , ğ‘¥ (1), â€¦ , ğ‘¥ (ğ¿) = ğ‘ ğ‘ |ğ‘¥ (1)

ğ‘ ğ‘¥ (ğ‘™)|ğ‘¥ (ğ‘™+1)

ğ‘ ğ‘¥ (ğ¿)

ğ‘™=1

Minimize Local Free-Energy at every layer
ğ¹ ğ‘™ = ğ”¼ğ‘ ğ‘¥ (ğ‘™) , ğ‘¥ (ğ‘™+1) ln ğ‘ ğ‘¥ (ğ‘™) âˆ’ ln ğ‘ ğ‘¥ (ğ‘™) |ğ‘¥ (ğ‘™+1)

precisionweighted
prediction
errors
precisionweighted
prediction
errors
precisionweighted
prediction
errors

Belief updating
updates hyperparameters
though the Global
Hyper-Model

Hyper-parameters Î¦ = Ï•(1) , â€¦ , Ï•(L)
modulate predictions of precision; and these
modulate the 'phenomenal optical properties'
of the layer in question from phenomenally
transparent to phenomenally opaque leading
to a modulation in epistemic depth

predictions
of precision

predictions
of precision

predictions
of precision

Note. This diagram illustrates the abstraction hierarchy of features as being composed of layers of 'smart' glass which have the
property of being able to change from being transparent (e.g., the layers on the left) to more or less opaque (e.g., the layers on the
right) analogizing phenomenal transparency or opacity of that layer. By analogy, when a pane of glass is opaque, the contents of
our consciousness are known (such as being aware of the feeling of wearing a shirt). On the other hand, when it is transparent, we
do not notice the shirtâ€”like looking through a clean window where we do not notice the glass. To account for this within
hierarchical active inference, we propose the following: The (local) free energy of every layer of the multilayer generative process
is minimized in the usual way, but as a crucial extension, global free energy is minimized in the context of a Global Hyper-Model
which includes a set of hyperparameters Î¦ = #ğœ™ (") , â€¦ , ğœ™ ($) ' that control predictions of precisions at every layer. These
hyperparameter controlled precision modulations can be said to regulate the 'phenomenal optical properties' of the layer in question
from phenomenally transparent to phenomenally opaque leading to a fully endogenously determined modulation of epistemic depth
globally. We unpack this further below and provide details in Table 1.

Unlike parametric depth, a hyper-model is modeling the very shape of its hierarchy and updating it in
real-time. Parametric depth is often implemented as localized loopsâ€”between one layer above another
(Sandved-Smith, 2021; 2024), implementing a second-order inference about attention, or about preference
precision. One can extend this idea to multiple layers, but typically it is demonstrated with a single or small
number of layers. Epistemic depth goes beyond local second-order inferences, implying a globally
consistent sense that "I (the system) have a multi-tier generative model, and I know how to deploy the right
precision in each tierâ€”thus I know what I know." This kind of epistemic depth is system-wide: it is not just
"what am I attending to?" but "how do all these layers of inference contextualize each other in a deep

(hierarchical) sense?" Whereas parametric depth can be instantiated with a few carefully chosen
parameters: e.g., "likelihood precision" or "policy precision" (Allen et al., 2019; Hesp et al., 2019; Parr and
Friston, 2017, 2019; Schwartenbeck et al., 2015; Smith et al., 2019), epistemic depth is about the entire
deep generative model being "aware" of how it's orchestrating priors, transitions, preferences, timescales,
and so on. Nevertheless, parametric and epistemic depth are clearly compatibleâ€”epistemic depth sketches
the 'big-picture' of global awareness, while parametric depth is a mechanism for implementing metainference in a hierarchical generative model that has close connections to metacognition and the higher-order thought theory (Fleming, 2020; Fleming et al., 2012).
Table 1
Towards a Formal Model of Epistemic Depth

Note. A simplified description of each component is as follows. Multilayer Generative Process: This is the standard hierarchical
formulation found in active inference and predictive coding. Global Hyper-Model: The novelty here is to include an extra "layer"
of inference that regulates how each level in the hierarchy should be trustedâ€”by updating the hyper-parameters Î¦ that set precision
and weighting rules across levels. This reflective control is our formal analog of epistemic depth. Local and Hyper Free-Energy:
The free-energy formalism provides a way to quantify "prediction error" at both local and global levels, where updates are shared
recursively between local and global levels. The local free-energy drives short-term, layer-by-layer inference, whereas the hyper
free-energy ensures that the whole hierarchy (including its meta-parameters) is optimized. These local and global processes may
be what give rise to the sense that there is a difference between awareness and its contents: The global level (awareness) always
seems to track the functioning and structure of other, localized loops (contents) in the context of the whole.

It also remains an open question where, along the phylogenetic continuum, genuine epistemic depth
begins to appear. Biologically, all living systems engage in some form of homeostatic regulation, and many
(e.g., bacteria) exhibit simple feedback loops. However, the simplest formal demonstration of epistemic
depth is probably a two(+)-layer active-inference system that:
1.
2.

Infers external states (a minimal world model - the "what" of consciousness)
Maintains a meta-layer that infers "confidence about those inferences" (minimal competition
between possible interpretations of the causes of sensation)

14

3.

Reflectively modifies the lower-level inference from the meta-layer's vantage, creating a
closed loop of self-modeling (minimal epistemic depth)6

Even such a toy system can, in principle, encode a rudimentary "knowing that it knows." This simple
setup forms a minimal demonstration of epistemic depth: the global loop includes not just a model of the
world, but also a real-time model of how it is modeling the world (cf. Parr & Friston, 2018; Sandved-Smith
et al., 2021). However, unlike reflecting on how one's mind works (in the way we are doing here),
rudimentary forms of epistemic depth are exceptionally basic: they involve minimal meta-inference about
a system's own predictive processes, absent any richer conceptual or introspective dimension. In this sense,
our model seems to suggest that consciousness clearly precedes introspection or complex metacognition, at
least the kind that we usually associate with those terms. Even a tiny agent can incorporate ongoing
feedback from within its own inferential machinery, linking the "sense of the world" with a subtle, selfrevising "sense of itself as the world." Self-modeling proper (i.e., knowing what kind of thing one is) would
be, under this view, a much later development.
In nature, many single-cell organisms already show proto-forms of "self-measurement" of intracellular
states, but whether that amounts to a reflective awareness is debatable (Fields & Levin, 2022; 2023). One
could argue that very small multicellular creatures or tiny insectsâ€”e.g. parasitic wasps (Megaphragma
mymaripenne), fruit-fly larvae (Drosophila melanogaster), or nematodes like C. elegansâ€”start to approach
the complexity needed to implement the minimal hyper-model. These small but highly integrated nervous
systems can tune sensory signals, modulate action policies, and reconfigure local loops via
neuromodulators, suggesting partial analogs of top-down reweighting (Marder, 2012). Whether these are
enough for a globally coherent "knowing that they know" remains unknown, but they are prime targets for
studying borderline cases of reflective, multi-level organization in living systems7. As a practical matter, it
may be that only once a creature devotes sufficient neuronal (or computational) resources to hierarchical
modelling and meta-inference do we see a clear approximation of epistemic depth in the sense required here
(Friston, 2018). Nonetheless, tracking how progressively complex nervous systemsâ€”beginning even with
tiny arthropodsâ€”handle global precision control may shed light on how minimal systems might, at least in
principle, exhibit the core properties of epistemic depth. 